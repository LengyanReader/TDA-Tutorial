{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will sample points from two different two dimensional normal distributions (difference will be in their covariance matrices). We will initially sample 10 point clouds from each distributions. For each point cloud, we will compute its Alpha complex and its persistent homology. The persistence will be stored and subsequently we will try to classify distributions based on their persistence signature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gudhi as gd\n",
    "import gudhi.representations\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the first distribution that we will consider\n",
    "mean_1 = [0, 0]\n",
    "cov_1 = [[1, 0], [0, 2]]\n",
    "number_of_points = 500\n",
    "x, y = np.random.multivariate_normal(mean_1, cov_1, number_of_points).T\n",
    "\n",
    "#plot it:\n",
    "plt.plot(x.T, y.T, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the second distribution that we will consider\n",
    "mean_2 = [10, 20]\n",
    "cov_2 = [[1, 0], [0, 1]]\n",
    "x2, y2 = np.random.multivariate_normal(mean_2, cov_2, number_of_points).T\n",
    "\n",
    "#plot it:\n",
    "plt.plot(x2.T, y2.T, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, those two point clouds are different, and they span different range. Give this, clearly zero dimensional persistet homology will be able to tell them apart. But, since we do not want to make it so easy, we will normalize the point clouds below so that an average distance between a pair of points is 1. In what follows, we will generate ten point clouds from each distribution and compute their persistent homology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will store diagrams and labels\n",
    "dgms, labs = [], []\n",
    "number_of_repetitions_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will generate the point cloud again, normalize it, and re-run the persistent homology information:\n",
    "\n",
    "for cls in range(0,number_of_repetitions_per_class):\n",
    "    x, y = np.random.multivariate_normal(mean_1, cov_1, number_of_points).T\n",
    "    \n",
    "    #Now compute average distance between points \n",
    "    sum_ = 0\n",
    "    count = 0\n",
    "    for i in range( 0,len(x) ):\n",
    "        for j in range( i+1,len(y) ):\n",
    "            #Compute Euclidean distance between (x[i],y[i]) and (x[j],y[j])\n",
    "            dst = math.sqrt( (x[i]-x[j])**2 + (y[i]-y[j])**2 )\n",
    "            sum_ = sum_+dst\n",
    "            count = count+1 \n",
    "    average_distance = sum_/count;\n",
    "\n",
    "    #And normalize the point clouds:\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = x[i] / average_distance\n",
    "    for i in range(0,len(y)):\n",
    "        y[i] = y[i] / average_distance\n",
    "\n",
    "    #Prepare points for Gudhi\n",
    "    pts1 = np.array([x,y]).T\n",
    "    pts1 = pts1.tolist();\n",
    "\n",
    "    simplex_tree = gd.AlphaComplex(points=pts1).create_simplex_tree()\n",
    "    pers = simplex_tree.persistence()\n",
    "    plt = gd.plot_persistence_diagram( pers )\n",
    "    dgms.append(simplex_tree.persistence_intervals_in_dimension(1))\n",
    "    labs.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#and now the same for mean_2 and cov_2:\n",
    "for cls in range(0,number_of_repetitions_per_class):\n",
    "    x, y = np.random.multivariate_normal(mean_2, cov_2, number_of_points).T\n",
    "    \n",
    "    #Now compute average distance between points \n",
    "    sum_ = 0\n",
    "    count = 0\n",
    "    for i in range( 0,len(x) ):\n",
    "        for j in range( i+1,len(y) ):\n",
    "            #Compute Euclidean distance between (x[i],y[i]) and (x[j],y[j])\n",
    "            dst = math.sqrt( (x[i]-x[j])**2 + (y[i]-y[j])**2 )\n",
    "            sum_ = sum_+dst\n",
    "            count = count+1 \n",
    "    average_distance = sum_/count;\n",
    "\n",
    "    #And normalize the point clouds:\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = x[i] / average_distance\n",
    "    for i in range(0,len(y)):\n",
    "        y[i] = y[i] / average_distance\n",
    "\n",
    "    #Prepare points for Gudhi\n",
    "    pts1 = np.array([x,y]).T\n",
    "    pts1 = pts1.tolist();\n",
    "\n",
    "    simplex_tree = gd.AlphaComplex(points=pts1).create_simplex_tree()\n",
    "    pers = simplex_tree.persistence()\n",
    "    plt = gd.plot_persistence_diagram( pers )\n",
    "    dgms.append(simplex_tree.persistence_intervals_in_dimension(1))\n",
    "    labs.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classification code was taken from:\n",
    "https://github.com/GUDHI/TDA-tutorial/blob/master/Tuto-GUDHI-representations.ipynb\n",
    "by Mathieu Carri√®re and Vincent Rouvreau. Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size            = 0.5\n",
    "perm                 = np.random.permutation(len(labs))\n",
    "limit                = np.int(test_size * len(labs))\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "train_labs           = np.array(labs)[train_sub]\n",
    "test_labs            = np.array(labs)[test_sub]\n",
    "train_dgms           = [dgms[i] for i in train_sub]\n",
    "test_dgms            = [dgms[i] for i in test_sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing   import MinMaxScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "# Definition of pipeline\n",
    "pipe = Pipeline([(\"Separator\", gd.representations.DiagramSelector(limit=np.inf, point_type=\"finite\")),\n",
    "                 (\"Scaler\",    gd.representations.DiagramScaler(scalers=[([0,1], MinMaxScaler())])),\n",
    "                 (\"TDA\",       gd.representations.PersistenceImage()),\n",
    "                 (\"Estimator\", SVC())])\n",
    "\n",
    "# Parameters of pipeline. This is the place where you specify the methods you want to use to handle diagrams\n",
    "param =    [{\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.SlicedWassersteinKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 1.0],\n",
    "             \"TDA__num_directions\": [20],\n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\", gamma=\"auto\")]},\n",
    "            \n",
    "            {\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.PersistenceWeightedGaussianKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 0.01],\n",
    "             \"TDA__weight\":         [lambda x: np.arctan(x[1]-x[0])], \n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\", gamma=\"auto\")]},\n",
    "            \n",
    "            {\"Scaler__use\":         [True],\n",
    "             \"TDA\":                 [gd.representations.PersistenceImage()], \n",
    "             \"TDA__resolution\":     [ [5,5], [6,6] ],\n",
    "             \"TDA__bandwidth\":      [0.01, 0.1, 1.0, 10.0],\n",
    "             \"Estimator\":           [SVC()]},\n",
    "            \n",
    "            {\"Scaler__use\":         [True],\n",
    "             \"TDA\":                 [gd.representations.Landscape()], \n",
    "             \"TDA__resolution\":     [100],\n",
    "             \"Estimator\":           [RandomForestClassifier()]},\n",
    "           \n",
    "            {\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.BottleneckDistance()], \n",
    "             \"TDA__epsilon\":        [0.1], \n",
    "             \"Estimator\":           [KNeighborsClassifier(metric=\"precomputed\")]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GridSearchCV(pipe, param, cv=3)\n",
    "model = model.fit(train_dgms, train_labs)\n",
    "print(model.best_params_)\n",
    "print(\"Train accuracy = \" + str(model.score(train_dgms, train_labs)))\n",
    "print(\"Test accuracy  = \" + str(model.score(test_dgms,  test_labs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
